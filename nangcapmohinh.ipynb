{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f73dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Dữ liệu gốc: (420551, 15)\n",
      "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
      "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
      "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
      "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
      "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
      "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
      "\n",
      "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
      "0          3.33          3.11          0.22       1.94             3.12   \n",
      "1          3.23          3.02          0.21       1.89             3.03   \n",
      "2          3.21          3.01          0.20       1.88             3.02   \n",
      "3          3.26          3.07          0.19       1.92             3.08   \n",
      "4          3.27          3.08          0.19       1.92             3.09   \n",
      "\n",
      "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0       1307.75      1.03           1.75     152.3  \n",
      "1       1309.80      0.72           1.50     136.1  \n",
      "2       1310.24      0.19           0.63     171.6  \n",
      "3       1309.19      0.34           0.50     198.0  \n",
      "4       1309.00      0.32           0.63     214.3  \n",
      "Tập mẫu: (420455, 72, 4) (420455, 24)\n",
      "Train: (294318, 72, 4), Val: (63068, 72, 4), Test: (63069, 72, 4)\n"
     ]
    }
   ],
   "source": [
    "import os, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Đọc dữ liệu Jena Climate\n",
    "df = pd.read_csv(\"jena_climate_2009_2016.csv\")\n",
    "print(\"Dữ liệu gốc:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Chỉ dùng một số cột (ví dụ: nhiệt độ, áp suất, gió)\n",
    "cols = [\"T (degC)\", \"p (mbar)\", \"wv (m/s)\", \"max. wv (m/s)\"]\n",
    "data = df[cols].values\n",
    "\n",
    "# Chuẩn hóa\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Hàm tạo chuỗi dữ liệu (input_len = số ngày quan sát, target_len = số ngày dự báo)\n",
    "def create_sequences(dataset, input_len=72, target_len=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - input_len - target_len):\n",
    "        X.append(dataset[i:i+input_len])\n",
    "        y.append(dataset[i+input_len:i+input_len+target_len, 0])  # Dự báo nhiệt độ\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(data_scaled, 72, 24)\n",
    "print(\"Tập mẫu:\", X.shape, y.shape)\n",
    "\n",
    "# Chia train / val / test\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float()),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(X_val).float(), torch.tensor(y_val).float()),\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float()),\n",
    "                         batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b28de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_and_eval(model, train_loader, val_loader, device, epochs=10, lr=1e-3, target_len=24, model_name='model', teacher_forcing=0.0):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    history = {'train_loss':[], 'val_mae':[], 'val_rmse':[]}\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if hasattr(model, \"forward\"):  # hỗ trợ cả Seq2SeqAttn & Transformer\n",
    "                if \"teacher_forcing_ratio\" in model.forward.__code__.co_varnames:\n",
    "                    outputs = model(x_batch, target_len, teacher_forcing_ratio=teacher_forcing, y_true=y_batch)\n",
    "                else:\n",
    "                    outputs = model(x_batch, target_len)\n",
    "            else:\n",
    "                outputs = model(x_batch, target_len)\n",
    "\n",
    "            if outputs.dim() == 3:\n",
    "                outputs = outputs.squeeze(-1)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                out = model(x_val, target_len)\n",
    "                if out.dim() == 3:\n",
    "                    out = out.squeeze(-1)\n",
    "                preds.append(out.cpu().numpy())\n",
    "                trues.append(y_val.cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        trues = np.concatenate(trues)\n",
    "        mae = mean_absolute_error(trues, preds)\n",
    "        rmse = math.sqrt(mean_squared_error(trues, preds))\n",
    "        history['val_mae'].append(mae)\n",
    "        history['val_rmse'].append(rmse)\n",
    "\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f} | Time: {time.time()-t0:.1f}s\")\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{model_name}_epoch{epoch+1}.pt\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_attention import Seq2SeqAttn\n",
    "from transformer import TimeSeriesTransformer\n",
    "\n",
    "input_dim = X_train.shape[2]\n",
    "target_len = y_train.shape[1]\n",
    "\n",
    "# 1️⃣ LSTM cơ bản (không Attention)\n",
    "lstm_base = Seq2SeqAttn(input_dim=input_dim, hidden_dim=32, output_dim=1, num_layers=1)\n",
    "hist_lstm = train_and_eval(lstm_base, train_loader, val_loader, device, epochs=5, lr=1e-3,\n",
    "                           target_len=target_len, model_name='LSTM_BASE', teacher_forcing=0.0)\n",
    "\n",
    "# 2️⃣ LSTM + Attention\n",
    "lstm_attn = Seq2SeqAttn(input_dim=input_dim, hidden_dim=32, output_dim=1, num_layers=1)\n",
    "hist_attn = train_and_eval(lstm_attn, train_loader, val_loader, device, epochs=5, lr=1e-3,\n",
    "                           target_len=target_len, model_name='LSTM_ATTN', teacher_forcing=0.2)\n",
    "\n",
    "# 3️⃣ Transformer\n",
    "trans_model = TimeSeriesTransformer(input_dim=input_dim, d_model=64, nhead=4,\n",
    "                                    num_encoder_layers=2, num_decoder_layers=2,\n",
    "                                    dim_feedforward=128, dropout=0.1,\n",
    "                                    target_len=target_len, output_dim=1)\n",
    "hist_trans = train_and_eval(trans_model, train_loader, val_loader, device, epochs=5, lr=1e-3,\n",
    "                            target_len=target_len, model_name='TRANSFORMER')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
